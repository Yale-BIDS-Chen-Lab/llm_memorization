task: llm-sft
base_model: xx
project_name: model_name_lr1e6_bs16
log: tensorboard
backend: local-cli

data:
  path: xx
  train_split: train
  valid_split: valid
  chat_template: chatml
  column_mapping:
    text_column: conversations

params:
  text_column: conversations

  block_size: 2048
  model_max_length: 2048
  early_stopping: true
  early_stopping_patience: 3 
  early_stopping_threshold: 0.0 
  metric: "eval_loss" 

  epochs: 10
  batch_size: 16
  lr: 1e-6
  weight_decay: 0.1

  save_strategy: epoch
  save_total_limit: 10

  peft: true
  lora_r: 64
  lora_alpha: 128
  lora_dropout: 0.0
  quantization: int4
  target_modules: all-linear

  optimizer: adamw_torch
  scheduler: cosine
  gradient_accumulation: 1
  mixed_precision: bf16
  padding: right
