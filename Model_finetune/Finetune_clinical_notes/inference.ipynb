{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d065b3-cee0-4f59-b897-9c314f76aed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_name =  '../../saved_models/Llama3_70B_Instruct/'  # PATH TO LLM \n",
    "\n",
    "# GET original file names for store results\n",
    "folder_path =  'example_test/'   #'CHANGE TO PATH OF THE DATA FOR BIO Test files\n",
    "files = [f for f in os.listdir(folder_path) if f.endswith('.bio')]\n",
    "\n",
    "# load processed data\n",
    "data = load_dataset('/home/jupyter/20000360102458359xu/LingfeiQian/saved_dataset/YBXL/Bilingual_example_test/') # Path for processed huggingface format test dataset\n",
    "out_dir = \"llm_results\" # path to output files\n",
    "\n",
    "gpu_number = 2 # GPU number\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "sampling_params = SamplingParams(max_tokens=512,stop='<EOS>',temperature=0)\n",
    "llm = LLM(model=f\"{model_name}\", tensor_parallel_size = gpu_number, dtype=torch.bfloat16,device = 'auto',max_model_len=2000)  # Create an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c72fa7bb-e701-4983-99f3-ffbef4af0a50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T21:10:28.082152Z",
     "iopub.status.busy": "2025-04-23T21:10:28.081896Z",
     "iopub.status.idle": "2025-04-23T21:10:28.087480Z",
     "shell.execute_reply": "2025-04-23T21:10:28.086601Z",
     "shell.execute_reply.started": "2025-04-23T21:10:28.082123Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_list(input_list, batch_size):\n",
    "    batched_list = []\n",
    "    for i in range(0, len(input_list), batch_size):\n",
    "        batched_list.append(input_list[i:i + batch_size])\n",
    "    return batched_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaabec49-f450-4f0b-a2ee-b3d340d59ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "# USE LLMS TO INFERENCE\n",
    "prompts_list = batch_list(data['test']['query'], batch_size)\n",
    "\n",
    "outputs = []\n",
    "for i,prompt_list in enumerate(prompts_list):\n",
    "    print (f'batch:{i+1} of total:{len(prompts_list)}', flush=True)\n",
    "    output = llm.generate(prompt_list,sampling_params)\n",
    "    outputs += output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f88c79b-629a-4f73-953b-4978f0e4a89a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FOR NER TASK\n",
    "! mkdir out_dir\n",
    "for i, (seq, file_name) in enumerate(zip(outputs,files)):\n",
    "    file_name = file_name.replace('.bio','')\n",
    "    with open(f'{out_dir}/{file_name}.html','w',encoding='utf-8') as f_write:\n",
    "        f_write.write(seq.outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe8250e-586f-45b7-9963-ef96fafbecd4",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1da46e05-e1f2-4173-aa98-30de2311259c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T21:11:21.762907Z",
     "iopub.status.busy": "2025-04-23T21:11:21.761664Z",
     "iopub.status.idle": "2025-04-23T21:11:25.570457Z",
     "shell.execute_reply": "2025-04-23T21:11:25.569397Z",
     "shell.execute_reply.started": "2025-04-23T21:11:21.762856Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from bs4 import NavigableString, Tag\n",
    "from glob import glob\n",
    "import spacy\n",
    "import random,os\n",
    "import pandas as pd\n",
    "import time\n",
    "from ner_metrics import classification_report\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "\n",
    "py_nlp = spacy.load (\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2967f5f2-22c7-439d-8bd5-b57f03a47864",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T21:11:33.932563Z",
     "iopub.status.busy": "2025-04-23T21:11:33.931810Z",
     "iopub.status.idle": "2025-04-23T21:11:33.941002Z",
     "shell.execute_reply": "2025-04-23T21:11:33.939616Z",
     "shell.execute_reply.started": "2025-04-23T21:11:33.932514Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_then_concatnate_tokens(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    merged_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.startswith('##'):\n",
    "            # If the token starts with '##', merge it with the previous token (remove '##' and concatenate)\n",
    "            merged_tokens[-1] += token[2:]\n",
    "        else:\n",
    "            # Otherwise, add the token as a new element in the list\n",
    "            merged_tokens.append(token)\n",
    "\n",
    "    # Join the tokens with a space to form a sentence\n",
    "    merged_sentence = ' '.join(merged_tokens)\n",
    "    return merged_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4092aa23-0d6e-485e-9b9f-f5b0bab5a401",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T21:11:34.204261Z",
     "iopub.status.busy": "2025-04-23T21:11:34.203188Z",
     "iopub.status.idle": "2025-04-23T21:11:34.217415Z",
     "shell.execute_reply": "2025-04-23T21:11:34.216116Z",
     "shell.execute_reply.started": "2025-04-23T21:11:34.204215Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bio2html(file):\n",
    "    with open(file,'r') as f_read:\n",
    "        lines = f_read.readlines()\n",
    "    \n",
    "    processed_text = ''\n",
    "    for i, line in enumerate(lines):\n",
    "        token, e_type = line.strip().split('\\t')\n",
    "        if e_type == 'O':\n",
    "            processed_text += token+' '\n",
    "            \n",
    "        if e_type.startswith('B-'):\n",
    "            if i <= len(lines)-2:\n",
    "                if lines[i+1]=='\\n' or lines[i+1].strip().split('\\t')[1]=='O' or lines[i+1].strip().split('\\t')[1].startswith('B-'):\n",
    "                    processed_text += f'<span class=\"{e_type[2:]}\">'+token+'</span> '\n",
    "                else:\n",
    "                    processed_text += f'<span class=\"{e_type[2:]}\">'+token+' '\n",
    "            else:\n",
    "                processed_text += f'<span class=\"{e_type[2:]}\">'+token+'</span> '\n",
    "            \n",
    "        if e_type.startswith('I-'):\n",
    "            if i <= len(lines)-2:\n",
    "                if lines[i+1]=='\\n' or lines[i+1].strip().split('\\t')[1]=='O' or lines[i+1].strip().split('\\t')[1].startswith('B-'):\n",
    "                    processed_text += token+'</span> '\n",
    "                else:\n",
    "                    processed_text += token+' '\n",
    "            else:\n",
    "                processed_text += token+'</span> '\n",
    "    return processed_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89da1a4c-71c4-46f8-8a2c-0dc85557b944",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T21:11:34.359713Z",
     "iopub.status.busy": "2025-04-23T21:11:34.359058Z",
     "iopub.status.idle": "2025-04-23T21:11:34.370461Z",
     "shell.execute_reply": "2025-04-23T21:11:34.369480Z",
     "shell.execute_reply.started": "2025-04-23T21:11:34.359671Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def html2bio(html_path,entity_list):\n",
    "    with open(html_path) as f:\n",
    "        \n",
    "        html = f.read()\n",
    "        \n",
    "        # Parse HTML using BeautifulSoup\n",
    "        soup = bs(html, \"html.parser\")\n",
    "\n",
    "        # Extract text under 'p' tags and convert to BIO format\n",
    "        bio_format = []\n",
    "        \n",
    "\n",
    "        for child in soup.children:\n",
    "            if isinstance(child, NavigableString):\n",
    "                child = split_then_concatnate_tokens(child)\n",
    "                for word in child.split():\n",
    "                    bio_format.append(f\"{word}\\tO\\n\")\n",
    "            elif isinstance(child, Tag):\n",
    "                words = split_then_concatnate_tokens(child.get_text()).split()\n",
    "                try:\n",
    "                    entity = child.attrs['class'][0]\n",
    "                except:\n",
    "                    entity = 'O'\n",
    "                if len(words) != 0:\n",
    "                    if entity != 'O' and entity in entity_list:\n",
    "                        bio_format.append(f\"{words[0]}\\tB-{entity}\\n\")\n",
    "                        for word in words[1:]:\n",
    "                            bio_format.append(f\"{word}\\tI-{entity}\\n\")\n",
    "                    else:\n",
    "                        bio_format.append(f\"{words[0]}\\tO\\n\")\n",
    "                        for word in words[1:]:\n",
    "                            bio_format.append(f\"{word}\\tO\\n\")\n",
    "    return bio_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b237a3-0ec2-48a6-a296-13352c3577e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from transformers import AutoTokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\", use_fast=True)\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, WordPunctTokenizer, RegexpTokenizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "tokenizer = RegexpTokenizer(r'\\w+|[/;\\-]|[^\\w\\s]', flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adffbcaf-c4c9-4a3e-ad65-680a06f3a98f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T21:11:35.887851Z",
     "iopub.status.busy": "2025-04-23T21:11:35.887197Z",
     "iopub.status.idle": "2025-04-23T21:11:35.899645Z",
     "shell.execute_reply": "2025-04-23T21:11:35.898837Z",
     "shell.execute_reply.started": "2025-04-23T21:11:35.887832Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ner_metrics import classification_report\n",
    "\n",
    "def get_performance(files, entity_list):\n",
    "    all_pre_tags = []\n",
    "    all_tokens = []\n",
    "    all_gold_tags = []\n",
    "\n",
    "    for file in files:\n",
    "        file_name = file.split('/')[-1].split('.')[0]\n",
    "        gold_html = bio2html(file)\n",
    "        with open('./tmp_html.html', 'w') as f:\n",
    "            f.write(gold_html)\n",
    "        gold_bio = html2bio('./tmp_html.html', entity_list)\n",
    "        tokens = [line.strip().split('\\t')[0] for line in gold_bio]\n",
    "        tags = [line.strip().split('\\t')[-1] for line in gold_bio]\n",
    "\n",
    "        prediction = f'{out_dir}/{file_name}.html'\n",
    "        bio_2 = html2bio(prediction, entity_list)\n",
    "        pre_tokens = [line.strip().split('\\t')[0] for line in bio_2]\n",
    "        pre_tags = [line.strip().split('\\t')[-1] for line in bio_2]\n",
    "        all_tokens += tokens\n",
    "\n",
    "        if len(gold_bio) == len(bio_2):\n",
    "            for token, gold_tag, pre_tag in zip(tokens, tags, pre_tags):\n",
    "                all_gold_tags.append(gold_tag)\n",
    "                all_pre_tags.append(pre_tag)\n",
    "        else:\n",
    "            for i, token in enumerate(tokens):\n",
    "                if token != '':\n",
    "                    match = False\n",
    "                    for i2 in range(i, -1, -1):\n",
    "                        try:\n",
    "                            token_2, tag_2 = bio_2[i2].strip().split('\\t')\n",
    "                        except:\n",
    "                            token_2, tag_2 = None, None\n",
    "                        if token_2 is not None:\n",
    "                            if token in token_2 or token_2 in token:\n",
    "                                match = True\n",
    "                                break\n",
    "                    if not match:\n",
    "                        tag_2 = 'O'\n",
    "                else:\n",
    "                    tag_2 = ''\n",
    "                all_gold_tags.append(tags[i])\n",
    "                all_pre_tags.append(tag_2)\n",
    "\n",
    "    # Get classification reports\n",
    "    lenient = classification_report(tags_true=all_gold_tags, tags_pred=all_pre_tags, mode=\"lenient\")\n",
    "    strict = classification_report(tags_true=all_gold_tags, tags_pred=all_pre_tags, mode=\"strict\")\n",
    "   # print(lenient)\n",
    "    data = []\n",
    "\n",
    "    for entity in strict.keys():\n",
    "        if entity == 'macro avg' or entity == 'micro avg':\n",
    "            continue  # skip these, we will handle overall separately\n",
    "        strict_scores = strict[entity]\n",
    "        lenient_scores = lenient.get(entity, {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0})\n",
    "        data.append({\n",
    "            'entity': entity,\n",
    "            'strict_precision': f\"{float(strict_scores['precision']):.4f}\",\n",
    "            'strict_recall': f\"{float(strict_scores['recall']):.4f}\",\n",
    "            'strict_f1-score': f\"{float(strict_scores['f1-score']):.4f}\",\n",
    "            'lenient_precision': f\"{float(lenient_scores['precision']):.4f}\",\n",
    "            'lenient_recall': f\"{float(lenient_scores['recall']):.4f}\",\n",
    "            'lenient_f1-score': f\"{float(lenient_scores['f1-score']):.4f}\",\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Display\n",
    "    print(df.to_string(index=False, justify='left', line_width=1000))\n",
    "    print()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b193dce6-104f-4e9c-92f9-9e5718a7ac19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#70b instruct\n",
    "entity_list = ['Language_Fluent','Language_Some','Language_Other','Language_No']\n",
    "\n",
    "folder_path =  'example_test'   #'CHANGE TO PATH OF THE DATA FOR NAMED ENTITY RECOGINITION, THE FORMAT IS BIO FILES IN THIS CASE'\n",
    "\n",
    "files = [folder_path+'/'+f for f in os.listdir(folder_path) if f.endswith('.bio')] # files for NER\n",
    "\n",
    "df = get_performance(files,entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2c01aa-ab45-49ef-a9ba-e7cb05e1ec51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inference",
   "language": "python",
   "name": "inference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
